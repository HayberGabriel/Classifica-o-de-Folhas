# =====================================================================================
# Script Completo para Projeto de Visão Computacional (Classificadores Tradicionais)
# Requisitos: Segmentação, Extracao de Descritores, PCA, KNN, SVM, Métricas de Avaliação
# =====================================================================================

# 2. Montar o Google Drive
from google.colab import drive
drive.mount('/content/drive')

# 3. Importar Bibliotecas Essenciais
import pandas as pd
import numpy as np
import cv2
import os
import shutil
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay

CSV_PATH = '/content/drive/MyDrive/Leaves/all.csv'
IMAGES_FOLDER = '/content/drive/MyDrive/Leaves'
ORGANIZED_FOLDER = '/content/leaves_dataset'

os.makedirs(ORGANIZED_FOLDER, exist_ok=True)

df = pd.read_csv(CSV_PATH)
if 'Unnamed: 0' in df.columns:
    df.drop('Unnamed: 0', axis=1, inplace=True)
df.rename(columns={'id': 'image_id', 'y': 'label'}, inplace=True)
df['image_id'] = df['image_id'].astype(str)
df['label'] = df['label'].astype(int)

train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])

def create_folder_structure(df_split, split_name):
    print(f"Criando pastas de {split_name}...")
    for idx, row in df_split.iterrows():
        class_folder = os.path.join(ORGANIZED_FOLDER, split_name, str(row['label']))
        os.makedirs(class_folder, exist_ok=True)
        src = os.path.join(IMAGES_FOLDER, row['image_id'])
        dst = os.path.join(class_folder, row['image_id'])
        if os.path.exists(src):
            if not os.path.exists(dst):
                shutil.copy2(src, dst)
        else:
            pass

create_folder_structure(train_df, 'train')
create_folder_structure(test_df, 'test')
print("Estrutura de pastas criada com sucesso!")

# =================================================================
# 5. Segmentação e Extração de Descritores
# =================================================================

def extract_features_from_image(image_path):
    # Carregar imagem em escala de cinza e colorida
    img_color = cv2.imread(image_path)
    img_gray = cv2.cvtColor(img_color, cv2.COLOR_BGR2GRAY)

    # --- A. Segmentação (Exemplo simples com Otsu para isolar folha do fundo) ---
    # Assumindo fundo branco/claro e folha escura
    _, thresh = cv2.threshold(img_gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    # Criar uma máscara (mask) para usar depois nos descritores de cor
    mask = thresh

    # --- B. Extração de Descritores ---

    # 1. Descritor de Forma/Contorno (Exemplo: Área e Perímetro normalizados)
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if contours:
        cnt = max(contours, key=cv2.contourArea)
        area = cv2.contourArea(cnt)
        perimeter = cv2.arcLength(cnt, True)
        if perimeter > 0:
            circularity = 4 * np.pi * area / (perimeter ** 2)
        else:
            circularity = 0
        shape_features = [area, perimeter, circularity]
    else:
        shape_features = [0, 0, 0]

    # 2. Descritor de Cor (Exemplo: Histograma de Cor no espaço HSV, usando a máscara)
    img_hsv = cv2.cvtColor(img_color, cv2.COLOR_BGR2HSV)
    hist_h = cv2.calcHist([img_hsv], [0], mask, [16], [0, 180]).flatten()
    hist_s = cv2.calcHist([img_hsv], [1], mask, [16], [0, 256]).flatten()
    hist_v = cv2.calcHist([img_hsv], [2], mask, [16], [0, 256]).flatten()
    color_features = np.concatenate([hist_h, hist_s, hist_v])

    # 3. Descritor de Textura

    # Combinar todos os descritores em um único vetor de features
    all_features = np.concatenate([shape_features, color_features])

    return all_features

def build_dataset_features(df_split, split_name):
    X, y = [], []
    for idx, row in df_split.iterrows():
        image_path = os.path.join(ORGANIZED_FOLDER, split_name, str(row['label']), row['image_id'])
        if os.path.exists(image_path):
            features = extract_features_from_image(image_path)
            X.append(features)
            y.append(row['label'])
    return np.array(X), np.array(y)

# Extrair features para treino e teste
print("\nIniciando extração de features de treino...")
X_train_raw, y_train = build_dataset_features(train_df, 'train')
print(f"Features de treino extraídas: {X_train_raw.shape}")

print("Iniciando extração de features de teste...")
X_test_raw, y_test = build_dataset_features(test_df, 'test')
print(f"Features de teste extraídas: {X_test_raw.shape}")

# Escalonar/Normalizar os dados (muito importante para PCA, KNN e SVM)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_raw)
X_test_scaled = scaler.transform(X_test_raw)

# =================================================================
# 6. Redução de Dimensionalidade (PCA) e Visualização
# =================================================================

# Justificativa do número de componentes (análise da variância explicada)
pca_full = PCA(n_components=None)
pca_full.fit(X_train_scaled)
cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)

# Plotar a variância acumulada para justificar a escolha (Requisito do Relatório)
plt.figure(figsize=(8, 5))
plt.plot(cumulative_variance)
plt.xlabel('Número de Componentes')
plt.ylabel('Variância Explicada Acumulada')
plt.title('Curva de Variância Explicada pelo PCA')
plt.grid(True)
# Salvar o gráfico para o relatório
plt.savefig('pca_variance_plot.png')
plt.show()

# Escolher número de componentes para reter 95% da variância
n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1
print(f"95% da variância é retida com {n_components_95} componentes.")

# Aplicar PCA com o número escolhido
pca = PCA(n_components=n_components_95)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

print(f"Dados após PCA: Treino {X_train_pca.shape}, Teste {X_test_pca.shape}")

# Visualização 2D/3D dos dados
if X_train_pca.shape[1] >= 2:
    plt.figure(figsize=(10, 8))
    # Usando os 2 primeiros componentes
    plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='viridis', alpha=0.5)
    plt.xlabel('Componente Principal 1')
    plt.ylabel('Componente Principal 2')
    plt.title('Visualização 2D dos Dados (PCA)')
    plt.colorbar(label='Classe')
    plt.savefig('pca_2d_visualization.png')
    plt.show()

# =================================================================
# 7. Classificação (KNN e SVM) e Avaliação
# =================================================================

def evaluate_model(y_true, y_pred, model_name):
    print(f"\n--- Avaliação do Modelo: {model_name} ---")
    print(f"Acurácia: {accuracy_score(y_true, y_pred):.4f}")
    print(f"Precisão (macro avg): {precision_score(y_true, y_pred, average='macro'):.4f}")
    print(f"Recall (macro avg): {recall_score(y_true, y_pred, average='macro'):.4f}")
    print(f"F1-Score (macro avg): {f1_score(y_true, y_pred, average='macro'):.4f}")

    # Matriz de Confusão (Requisito do Relatório)
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_true))
    disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')
    plt.title(f'Matriz de Confusão para {model_name}')
    plt.savefig(f'confusion_matrix_{model_name.lower()}.png')
    plt.show()

# --- A. Classificador KNN ---
print("\n===== Treinando KNN =====")
# Testar diferentes valores de K (Requisito: Curva de desempenho)
k_values = range(1, 21, 2) # Testando ímpares de 1 a 19
accuracies = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    # Usar validação cruzada para encontrar o melhor K
    scores = cross_val_score(knn, X_train_pca, y_train, cv=KFold(n_splits=5, shuffle=True, random_state=42), scoring='accuracy')
    accuracies.append(scores.mean())
    print(f"K={k}, Avg CV Accuracy: {scores.mean():.4f}")

# Plotar curva de desempenho KNN
plt.figure(figsize=(8, 5))
plt.plot(k_values, accuracies, marker='o')
plt.xlabel('Valor de K')
plt.ylabel('Acurácia Média da Validação Cruzada')
plt.title('Curva de Desempenho do KNN (Acurácia vs K)')
plt.grid(True)
plt.savefig('knn_performance_curve.png')
plt.show()

# Escolher o melhor K e treinar o modelo final
best_k = k_values[np.argmax(accuracies)]
print(f"Melhor K encontrado: {best_k}")

knn_final = KNeighborsClassifier(n_neighbors=best_k)
knn_final.fit(X_train_pca, y_train)
y_pred_knn = knn_final.predict(X_test_pca)

evaluate_model(y_test, y_pred_knn, "KNN")


# --- B. Classificador SVM ---
print("\n===== Treinando SVM =====")
# Testar kernels linear e RBF

# SVM Linear
svm_linear = SVC(kernel='linear', random_state=42)
svm_linear.fit(X_train_pca, y_train)
y_pred_svm_linear = svm_linear.predict(X_test_pca)
print("\nAvaliação SVM Linear:")
evaluate_model(y_test, y_pred_svm_linear, "SVM Linear")

# SVM RBF (Radial Basis Function)
svm_rbf = SVC(kernel='rbf', random_state=42)
svm_rbf.fit(X_train_pca, y_train)
y_pred_svm_rbf = svm_rbf.predict(X_test_pca)
print("\nAvaliação SVM RBF:")
evaluate_model(y_test, y_pred_svm_rbf, "SVM RBF")

print("\nScript concluído!")
print("Todos os gráficos e matrizes de confusão foram salvos como arquivos PNG no ambiente Colab local.")
